{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import boto3\n",
    "\n",
    "#cred = boto3.Session().get_credentials()\n",
    "\n",
    "# s3://cosmicenergy-ml-public-datasets/flight_data/raw_files/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_1.zip\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "s3_list = []\n",
    "for key in conn.list_objects_v2(Bucket=\"cosmicenergy-ml-public-datasets\",Prefix='flight_data/raw_files')['Contents']:\n",
    "    if (len(key['Key'].split(\"/\")) > 2) and (\"On_Time_Reporting\" in key['Key'].split(\"/\")[2]):\n",
    "        #print(key['Key'].split(\"/\")[2])\n",
    "        s3_list.append(key['Key'].split(\"/\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/tmp/cml_flight_demo/penv38/lib/python3.8/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'transtats.bts.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://transtats.bts.gov/PREZIP/\"\n",
    "req = requests.get(url,verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "len(soup.find_all(href=re.compile(\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "for year in range((today.year)-5,(today.year)+1):\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_list = []\n",
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "for year in range((today.year)-5,(today.year)+1):\n",
    "    for files in soup.find_all(href=re.compile(f\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{year}\")):\n",
    "        live_list.append(files.contents[0])\n",
    "    #print(files.contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(live_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = list(set(live_list).difference(set(s3_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_5.zip',\n",
       " 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_3.zip',\n",
       " 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_7.zip',\n",
       " 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_1.zip',\n",
       " 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_6.zip',\n",
       " 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_4.zip']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/tmp/cml_flight_demo/penv38/lib/python3.8/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'transtats.bts.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# THIS ONE!! \n",
    "#  Fetch the file from transtats and save locally\n",
    "import requests\n",
    "import zipfile\n",
    "from smart_open import open\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "filename = \"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_4\"\n",
    "\n",
    "URL = f\"https://transtats.bts.gov/PREZIP/{filename}.zip\"\n",
    "response = requests.get(URL,verify=False)\n",
    "\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as zip:\n",
    "    bob = zip.infolist()\n",
    "    for info in zip.infolist():\n",
    "        if \"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)\" in info.filename:\n",
    "            file_bytes = zip.read(info.filename)\n",
    "\n",
    "with open(f's3://AKIAYEO4VWCFD2PTXYI6:3nTjkTnsVejqUpQ/e5P87Lb215J4OmxXI/WdOHQo@jfletcher-datasets/flight_data/transtats/{filename}.csv.gz', 'wb') as fout:\n",
    "    fout.write(file_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f's3://AKIAYEO4VWCFD2PTXYI6:3nTjkTnsVejqUpQ/e5P87Lb215J4OmxXI/WdOHQo@jfletcher-datasets/flight_data/transtats/{filename}.csv.gz', 'wb') as fout:\n",
    "    fout.write(file_bytes)\n",
    "    # with gzip.GzipFile(fileobj=fout, mode='w') as f_zip:\n",
    "    #     f_zip.write(BytesIO(file_bytes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Year\",\"Quarter\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"FlightDate\",\"Reporting_Airline\",\"DOT_ID_Reporting'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_bytes.decode()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('ziper2.gz', 'wt') as f:\n",
    "    f.write(file_bytes.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "with open('zipper.gz','wb') as fout:\n",
    "        fout.write(file_bytes)\n",
    "    # with gzip.GzipFile(fileobj=fout,mode='w') as f_zip:\n",
    "    #     f_zip.write(BytesIO(file_bytes))\n",
    "\n",
    "    # with gzip.open(fout,'wb') as f:\n",
    "    #     f.write(file_bytes)\n",
    "    # g = gzip.GzipFile(fileobj=fout, mode='w')\n",
    "    # g.write(file_bytes.decode())\n",
    "    # g.close()\n",
    "#    with gzip.GzipFile(fileobj=fout, mode='w') as f_zip:\n",
    "#        f_zip.writelines(BytesIO(file_bytes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/b81vd85d6bs0qx8zh86b5m780000gn/T/ipykernel_64402/1751591345.py:3: DtypeWarning: Columns (76,77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/jeff/tmp/cml_flight_demo/data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/jeff/tmp/cml_flight_demo/data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'SH87MVX8Q6DSM1ZY',\n",
       "  'HostId': 'SGt1leezJ1OoTCKJamvs0kPhSNeWCjQonxpY7qzDj5XL3ywScOxUbqqyOK2W2INwBWeIcUvndmc=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'SGt1leezJ1OoTCKJamvs0kPhSNeWCjQonxpY7qzDj5XL3ywScOxUbqqyOK2W2INwBWeIcUvndmc=',\n",
       "   'x-amz-request-id': 'SH87MVX8Q6DSM1ZY',\n",
       "   'date': 'Tue, 18 Oct 2022 21:58:46 GMT',\n",
       "   'etag': '\"f06f6f027abefbb3409f5ecb78a83793\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"f06f6f027abefbb3409f5ecb78a83793\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import boto3\n",
    "\n",
    "cred = boto3.Session().get_credentials()\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "   aws_access_key_id = 'AKIAYEO4VWCFD2PTXYI6',\n",
    "   aws_secret_access_key = '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',\n",
    ")\n",
    "bucketname = 'jfletcher-datasets'\n",
    "key = 'flight_data/transtats/test.csv.gz'\n",
    "\n",
    "#s3.upload_fileobj(file_bytes, bucketname, key)\n",
    "s3.put_object(Body=gzip.compress(file_bytes),Bucket= bucketname,Key =key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                                 int64\n",
      "Quarter                              int64\n",
      "Month                                int64\n",
      "DayofMonth                           int64\n",
      "DayOfWeek                            int64\n",
      "FlightDate                          object\n",
      "Reporting_Airline                   object\n",
      "DOT_ID_Reporting_Airline             int64\n",
      "IATA_CODE_Reporting_Airline         object\n",
      "Tail_Number                         object\n",
      "Flight_Number_Reporting_Airline      int64\n",
      "OriginAirportID                      int64\n",
      "OriginAirportSeqID                   int64\n",
      "OriginCityMarketID                   int64\n",
      "Origin                              object\n",
      "OriginCityName                      object\n",
      "OriginState                         object\n",
      "OriginStateFips                      int64\n",
      "OriginStateName                     object\n",
      "OriginWac                            int64\n",
      "DestAirportID                        int64\n",
      "DestAirportSeqID                     int64\n",
      "DestCityMarketID                     int64\n",
      "Dest                                object\n",
      "DestCityName                        object\n",
      "DestState                           object\n",
      "DestStateFips                        int64\n",
      "DestStateName                       object\n",
      "DestWac                              int64\n",
      "CRSDepTime                           int64\n",
      "DepTime                            float64\n",
      "DepDelay                           float64\n",
      "DepDelayMinutes                    float64\n",
      "DepDel15                           float64\n",
      "DepartureDelayGroups               float64\n",
      "DepTimeBlk                          object\n",
      "TaxiOut                            float64\n",
      "WheelsOff                          float64\n",
      "WheelsOn                           float64\n",
      "TaxiIn                             float64\n",
      "CRSArrTime                           int64\n",
      "ArrTime                            float64\n",
      "ArrDelay                           float64\n",
      "ArrDelayMinutes                    float64\n",
      "ArrDel15                           float64\n",
      "ArrivalDelayGroups                 float64\n",
      "ArrTimeBlk                          object\n",
      "Cancelled                          float64\n",
      "CancellationCode                    object\n",
      "Diverted                           float64\n",
      "CRSElapsedTime                     float64\n",
      "ActualElapsedTime                  float64\n",
      "AirTime                            float64\n",
      "Flights                            float64\n",
      "Distance                           float64\n",
      "DistanceGroup                        int64\n",
      "CarrierDelay                       float64\n",
      "WeatherDelay                       float64\n",
      "NASDelay                           float64\n",
      "SecurityDelay                      float64\n",
      "LateAircraftDelay                  float64\n",
      "FirstDepTime                       float64\n",
      "TotalAddGTime                      float64\n",
      "LongestAddGTime                    float64\n",
      "DivAirportLandings                   int64\n",
      "DivReachedDest                     float64\n",
      "DivActualElapsedTime               float64\n",
      "DivArrDelay                        float64\n",
      "DivDistance                        float64\n",
      "Div1Airport                         object\n",
      "Div1AirportID                      float64\n",
      "Div1AirportSeqID                   float64\n",
      "Div1WheelsOn                       float64\n",
      "Div1TotalGTime                     float64\n",
      "Div1LongestGTime                   float64\n",
      "Div1WheelsOff                      float64\n",
      "Div1TailNum                         object\n",
      "Div2Airport                         object\n",
      "Div2AirportID                      float64\n",
      "Div2AirportSeqID                   float64\n",
      "Div2WheelsOn                       float64\n",
      "Div2TotalGTime                     float64\n",
      "Div2LongestGTime                   float64\n",
      "Div2WheelsOff                      float64\n",
      "Div2TailNum                         object\n",
      "Div3Airport                        float64\n",
      "Div3AirportID                      float64\n",
      "Div3AirportSeqID                   float64\n",
      "Div3WheelsOn                       float64\n",
      "Div3TotalGTime                     float64\n",
      "Div3LongestGTime                   float64\n",
      "Div3WheelsOff                      float64\n",
      "Div3TailNum                        float64\n",
      "Div4Airport                        float64\n",
      "Div4AirportID                      float64\n",
      "Div4AirportSeqID                   float64\n",
      "Div4WheelsOn                       float64\n",
      "Div4TotalGTime                     float64\n",
      "Div4LongestGTime                   float64\n",
      "Div4WheelsOff                      float64\n",
      "Div4TailNum                        float64\n",
      "Div5Airport                        float64\n",
      "Div5AirportID                      float64\n",
      "Div5AirportSeqID                   float64\n",
      "Div5WheelsOn                       float64\n",
      "Div5TotalGTime                     float64\n",
      "Div5LongestGTime                   float64\n",
      "Div5WheelsOff                      float64\n",
      "Div5TailNum                        float64\n",
      "Unnamed: 109                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):       print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jeff/tmp/cml_flight_demo/data\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/jeff/tmp/cml_flight_demo/data\")\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_3.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "with zipfile.ZipFile(\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_3.zip\", mode=\"w\") as archive:\n",
    "    archive.write(\"On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\",compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function open in smart_open:\n",
      "\n",
      "smart_open.open = open(uri, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None, compression='infer_from_extension', transport_params=None)\n",
      "    Open the URI object, returning a file-like object.\n",
      "    \n",
      "    The URI is usually a string in a variety of formats.\n",
      "    For a full list of examples, see the :func:`parse_uri` function.\n",
      "    \n",
      "    The URI may also be one of:\n",
      "    \n",
      "    - an instance of the pathlib.Path class\n",
      "    - a stream (anything that implements io.IOBase-like functionality)\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    uri: str or object\n",
      "        The object to open.\n",
      "    mode: str, optional\n",
      "        Mimicks built-in open parameter of the same name.\n",
      "    buffering: int, optional\n",
      "        Mimicks built-in open parameter of the same name.\n",
      "    encoding: str, optional\n",
      "        Mimicks built-in open parameter of the same name.\n",
      "    errors: str, optional\n",
      "        Mimicks built-in open parameter of the same name.\n",
      "    newline: str, optional\n",
      "        Mimicks built-in open parameter of the same name.\n",
      "    closefd: boolean, optional\n",
      "        Mimicks built-in open parameter of the same name.  Ignored.\n",
      "    opener: object, optional\n",
      "        Mimicks built-in open parameter of the same name.  Ignored.\n",
      "    compression: str, optional (see smart_open.compression.get_supported_compression_types)\n",
      "        Explicitly specify the compression/decompression behavior.\n",
      "    transport_params: dict, optional\n",
      "        Additional parameters for the transport layer (see notes below).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    A file-like object.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    smart_open has several implementations for its transport layer (e.g. S3, HTTP).\n",
      "    Each transport layer has a different set of keyword arguments for overriding\n",
      "    default behavior.  If you specify a keyword argument that is *not* supported\n",
      "    by the transport layer being used, smart_open will ignore that argument and\n",
      "    log a warning message.\n",
      "    \n",
      "    smart_open supports the following transport mechanisms:\n",
      "    \n",
      "    file (smart_open/local_file.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements the transport for the file:// schema.\n",
      "    \n",
      "    hdfs (smart_open/hdfs.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements reading and writing to/from HDFS.\n",
      "    \n",
      "    http (smart_open/http.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements file-like objects for reading from http.\n",
      "    \n",
      "    kerberos: boolean, optional\n",
      "        If True, will attempt to use the local Kerberos credentials\n",
      "    user: str, optional\n",
      "        The username for authenticating over HTTP\n",
      "    password: str, optional\n",
      "        The password for authenticating over HTTP\n",
      "    headers: dict, optional\n",
      "        Any headers to send in the request. If ``None``, the default headers are sent:\n",
      "        ``{'Accept-Encoding': 'identity'}``. To use no headers at all,\n",
      "        set this variable to an empty dict, ``{}``.\n",
      "    \n",
      "    s3 (smart_open/s3.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements file-like objects for reading and writing from/to AWS S3.\n",
      "    \n",
      "    buffer_size: int, optional\n",
      "        The buffer size to use when performing I/O.\n",
      "    min_part_size: int, optional\n",
      "        The minimum part size for multipart uploads.  For writing only.\n",
      "    multipart_upload: bool, optional\n",
      "        Default: `True`\n",
      "        If set to `True`, will use multipart upload for writing to S3. If set\n",
      "        to `False`, S3 upload will use the S3 Single-Part Upload API, which\n",
      "        is more ideal for small file sizes.\n",
      "        For writing only.\n",
      "    version_id: str, optional\n",
      "        Version of the object, used when reading object.\n",
      "        If None, will fetch the most recent version.\n",
      "    defer_seek: boolean, optional\n",
      "        Default: `False`\n",
      "        If set to `True` on a file opened for reading, GetObject will not be\n",
      "        called until the first seek() or read().\n",
      "        Avoids redundant API queries when seeking before reading.\n",
      "    client: object, optional\n",
      "        The S3 client to use when working with boto3.\n",
      "        If you don't specify this, then smart_open will create a new client for you.\n",
      "    client_kwargs: dict, optional\n",
      "        Additional parameters to pass to the relevant functions of the client.\n",
      "        The keys are fully qualified method names, e.g. `S3.Client.create_multipart_upload`.\n",
      "        The values are kwargs to pass to that method each time it is called.\n",
      "    writebuffer: IO[bytes], optional\n",
      "        By default, this module will buffer data in memory using io.BytesIO\n",
      "        when writing. Pass another binary IO instance here to use it instead.\n",
      "        For example, you may pass a file object to buffer to local disk instead\n",
      "        of in RAM. Use this to keep RAM usage low at the expense of additional\n",
      "        disk IO. If you pass in an open file, then you are responsible for\n",
      "        cleaning it up after writing completes.\n",
      "    \n",
      "    scp (smart_open/ssh.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements I/O streams over SSH.\n",
      "    \n",
      "    mode: str, optional\n",
      "        The mode to use for opening the file.\n",
      "    host: str, optional\n",
      "        The hostname of the remote machine.  May not be None.\n",
      "    user: str, optional\n",
      "        The username to use to login to the remote machine.\n",
      "        If None, defaults to the name of the current user.\n",
      "    password: str, optional\n",
      "        The password to use to login to the remote machine.\n",
      "    port: int, optional\n",
      "        The port to connect to.\n",
      "    transport_params: dict, optional\n",
      "        Any additional settings to be passed to paramiko.SSHClient.connect\n",
      "    \n",
      "    webhdfs (smart_open/webhdfs.py)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Implements reading and writing to/from WebHDFS.\n",
      "    \n",
      "    min_part_size: int, optional\n",
      "        For writing only.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    See README.rst\n",
      "    This function also supports transparent compression and decompression \n",
      "    using the following codecs:\n",
      "    \n",
      "    * .bz2\n",
      "    * .gz\n",
      "    \n",
      "    The function depends on the file extension to determine the appropriate codec.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    - `Standard library reference <https://docs.python.org/3.7/library/functions.html#open>`__\n",
      "    - `smart_open README.rst\n",
      "      <https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst>`__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('smart_open.open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_1.csv\n"
     ]
    }
   ],
   "source": [
    "from smart_open import open\n",
    "import zipfile\n",
    "\n",
    "with open('https://jfletcher-datasets.s3.eu-central-1.amazonaws.com/flight_data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_1.zip', 'rb') as fin:\n",
    "    with zipfile.ZipFile(fin) as zip:\n",
    "        for info in zip.infolist():\n",
    "            print(info.filename)\n",
    "            #file_bytes = zip.read(info.filename)\n",
    "            #print('%r: %r' % (info.filename, file_bytes.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# conn = boto3.client('s3')\n",
    "import boto3\n",
    "from smart_open import open\n",
    "#tp = {'min_part_size': 5 * 1024**2}\n",
    "with open(\"/Users/jeff/tmp/cml_flight_demo/data/On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2022_3.csv\",'r') as fin:\n",
    "    read_data = fin.read()\n",
    "\n",
    "#with open('s3://bucket/key', 'w') as fout:\n",
    "#    fout.write(lots_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('s3://AKIAYEO4VWCFD2PTXYI6:3nTjkTnsVejqUpQ/e5P87Lb215J4OmxXI/WdOHQo@jfletcher-datasets/flight_data/test.csv', 'w') as fout:\n",
    "    fout.write(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "import json\n",
    "#import pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/host/creds_2.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = {\n",
    "    'account': data['account'],\n",
    "    'user': data['username'],\n",
    "    'password': data['password'],\n",
    "    'role': data['role'],\n",
    "    'warehouse': data['warehouse'],\n",
    "    'database': data['database'],\n",
    "    'schema': data['schema']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = snp.Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.dataframe.DataFrame at 0xffff95db36a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"use sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"source\"                                            |\"target\"                                            |\"source_size\"  |\"target_size\"  |\"source_compression\"  |\"target_compression\"  |\"status\"  |\"message\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|On_Time_Reporting_Carrier_On_Time_Performance_1...  |On_Time_Reporting_Carrier_On_Time_Performance_1...  |24222778       |24228208       |NONE                  |GZIP                  |UPLOADED  |           |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"PUT 'file://On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_2.zip' @FLIGHT_DATA_STAGE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"name\"                                              |\"size\"     |\"md5\"                             |\"last_modified\"                |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|flight_data_stage/2009.csv.gz                       |206299776  |a8fef10a5c6eb655e44b4223dea68010  |Wed, 20 Jul 2022 14:21:03 GMT  |\n",
      "|flight_data_stage/2010.csv.gz                       |208423056  |eaf94007558aa0840b77b9a2d22c1433  |Wed, 20 Jul 2022 14:21:48 GMT  |\n",
      "|flight_data_stage/On_Time_Reporting_Carrier_On_...  |24228208   |02e6c2b60b129cbd012060ee48b94576  |Fri, 19 Aug 2022 14:51:52 GMT  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"LIST @FLIGHT_DATA_STAGE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/array/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m compute\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backends, fft, lib, linalg, ma, overlap, random\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mblockwise\u001b[39;00m \u001b[39mimport\u001b[39;00m atop, blockwise\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/array/backends.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnumpy_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m ma_divide\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpercentile\u001b[39;00m \u001b[39mimport\u001b[39;00m _percentile\n\u001b[1;32m     15\u001b[0m concatenate_lookup\u001b[39m.\u001b[39mregister((\u001b[39mobject\u001b[39m, np\u001b[39m.\u001b[39mndarray), np\u001b[39m.\u001b[39mconcatenate)\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/array/percentile.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhighlevelgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m HighLevelGraph\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Array\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnumpy_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m _numpy_122\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/array/core.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     DaskMethodsMixin,\n\u001b[1;32m     37\u001b[0m     compute_as_if_collection,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     tokenize,\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mblockwise\u001b[39;00m \u001b[39mimport\u001b[39;00m blockwise \u001b[39mas\u001b[39;00m core_blockwise\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mblockwise\u001b[39;00m \u001b[39mimport\u001b[39;00m broadcast_dimensions\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/blockwise.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone_key, get_name_from_key, tokenize\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m prod\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m flatten, keys_in_tasks, reverse_dict\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prod' from 'dask.compatibility' (/host/penv38-docker/lib/python3.8/site-packages/dask/compatibility.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/dataframe/__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m compute\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backends, dispatch, rolling\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     DataFrame,\n\u001b[1;32m      6\u001b[0m     Index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     to_timedelta,\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/dataframe/backends.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdispatch\u001b[39;00m \u001b[39mimport\u001b[39;00m percentile_lookup\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpercentile\u001b[39;00m \u001b[39mimport\u001b[39;00m _percentile\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msizeof\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleSizeof, sizeof\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/array/__init__.py:266\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    260\u001b[0m msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    261\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDask array requirements are not installed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease either conda or pip install as follows:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m  conda install dask                 # either conda install\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m  python -m pip install \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdask[array]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m --upgrade  # or python -m pip install\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    265\u001b[0m )\n\u001b[0;32m--> 266\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mstr\u001b[39m(e) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prod' from 'dask.compatibility' (/host/penv38-docker/lib/python3.8/site-packages/dask/compatibility.py)\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/host/local_testing.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6c6f76696e675f79616c6f77227d/host/local_testing.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ontime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/host/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_2.zip\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/logging/logger_function.py:65\u001b[0m, in \u001b[0;36mlogger_decorator.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     67\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/pandas/io.py:140\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    138\u001b[0m _, _, _, f_locals \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetargvalues(inspect\u001b[39m.\u001b[39mcurrentframe())\n\u001b[1;32m    139\u001b[0m kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m f_locals\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m _pd_read_csv_signature}\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m _read(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/pandas/io.py:57\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     45\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    Read csv file from local disk.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m    modin.pandas.DataFrame\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     Engine\u001b[39m.\u001b[39;49msubscribe(_update_engine)\n\u001b[1;32m     58\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdispatching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfactories\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdispatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m FactoryDispatcher\n\u001b[1;32m     60\u001b[0m     squeeze \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39msqueeze\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/config/pubsub.py:213\u001b[0m, in \u001b[0;36mParameter.subscribe\u001b[0;34m(cls, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mAdd `callback` to the `_subs` list and then execute it.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m    Callable to execute.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_subs\u001b[39m.\u001b[39mappend(callback)\n\u001b[0;32m--> 213\u001b[0m callback(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/pandas/__init__.py:138\u001b[0m, in \u001b[0;36m_update_engine\u001b[0;34m(publisher)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m _is_first_update\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mDask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    136\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_dask\n\u001b[0;32m--> 138\u001b[0m         initialize_dask()\n\u001b[1;32m    139\u001b[0m \u001b[39melif\u001b[39;00m publisher\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCloudray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m get_connection\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/modin/core/execution/dask/common/utils.py:22\u001b[0m, in \u001b[0;36minitialize_dask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_dask\u001b[39m():\n\u001b[1;32m     21\u001b[0m     \u001b[39m\"\"\"Initialize Dask environment.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m default_client\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         client \u001b[39m=\u001b[39m default_client()\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m config  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m get_versions\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mactor\u001b[39;00m \u001b[39mimport\u001b[39;00m Actor, ActorFuture\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Client,\n\u001b[1;32m     10\u001b[0m     CompatibleExecutor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     wait,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Status, connect, rpc\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/actor.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m Future\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m to_serialize\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m iscoroutinefunction, sync, thread_state\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/client.py:103\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m     All,\n\u001b[1;32m     81\u001b[0m     Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     thread_state,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils_comm\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     96\u001b[0m     WrappedKey,\n\u001b[1;32m     97\u001b[0m     gather_from_workers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     unpack_remotedata,\n\u001b[1;32m    102\u001b[0m )\n\u001b[0;32m--> 103\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mworker\u001b[39;00m \u001b[39mimport\u001b[39;00m get_client, get_worker, secede\n\u001b[1;32m    105\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    107\u001b[0m _global_clients: weakref\u001b[39m.\u001b[39mWeakValueDictionary[\n\u001b[1;32m    108\u001b[0m     \u001b[39mint\u001b[39m, Client\n\u001b[1;32m    109\u001b[0m ] \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mWeakValueDictionary()\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/worker.py:47\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msystem\u001b[39;00m \u001b[39mimport\u001b[39;00m CPU_COUNT\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     apply,\n\u001b[1;32m     39\u001b[0m     format_bytes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     typename,\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m comm, preloading, profile, shuffle, system, utils\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbatched\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchedSend\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcomm\u001b[39;00m \u001b[39mimport\u001b[39;00m Comm, connect, get_address_host\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/shuffle/__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mdel\u001b[39;00m pandas\n\u001b[1;32m      7\u001b[0m     SHUFFLE_AVAILABLE \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mshuffle\u001b[39;00m \u001b[39mimport\u001b[39;00m rearrange_by_column_p2p\n\u001b[1;32m     10\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mshuffle_extension\u001b[39;00m \u001b[39mimport\u001b[39;00m ShuffleId, ShuffleMetadata, ShuffleWorkerExtension\n\u001b[1;32m     12\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSHUFFLE_AVAILABLE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrearrange_by_column_p2p\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mShuffleWorkerExtension\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/distributed/shuffle/shuffle.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m tokenize\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdelayed\u001b[39;00m \u001b[39mimport\u001b[39;00m Delayed, delayed\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhighlevelgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m HighLevelGraph\n",
      "File \u001b[0;32m/host/penv38-docker/lib/python3.8/site-packages/dask/dataframe/__init__.py:62\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDask dataframe requirements are not installed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease either conda or pip install as follows:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m  conda install dask                     # either conda install\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m  python -m pip install \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdask[dataframe]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m --upgrade  # or python -m pip install\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "ontime = pd.read_csv('/host/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2022_2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODIN_ENGINE\"]='dask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('penv38': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2a604b4c491453c8470cafd886d5c5f5fd294a67e279b9a0bf6530ec8b8af6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
